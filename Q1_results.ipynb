{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marvik Technical Test \n",
    "\n",
    "```plaintext\n",
    "+-------------------+        +--------------------+        +-------------------+        +-------------------+\n",
    "|                   |        |                    |        |                   |        |                   |\n",
    "| AWS S3 Bucket     |        | Data Transformation|        | Generate Report   |        | Local File        |\n",
    "| (Input Data)      | -----> | (Filter by Quarter | -----> | (Per Quarter and  | -----> | (Output Report)   |\n",
    "| - Customer Data   |        |  and Traffic       |        |  Traffic Source)  |        |                   |\n",
    "| - Product Data    |        |  Source)           |        |                   |        |                   |\n",
    "| - Order Data      |        |                    |        |                   |        |                   |\n",
    "+-------------------+        +--------------------+        +-------------------+        +-------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the solution for it to be able to bring any year by input, including multiple years or even a time period of years (always bringing Q1 results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark==3.5.0 in /home/ibum/.local/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/ibum/.local/lib/python3.10/site-packages (from pyspark==3.5.0) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.5.0\n",
    "!wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar\n",
    "!wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.710/aws-java-sdk-bundle-1.12.710.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.functions import col, sum as _sum, count, lag, round\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with Q1 data of the selected year, plus the previous Quarter (so we can calculate the changes in number of purchases and profits from the previous period). \n",
    "Example:\n",
    "    We bring 2023 and 2024 data -> this will be Q1 data for 2023 and Q1 for 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    df = df.dropna(subset=[\"created_at_year\", \"created_at_month\", \"sale_price\", \"product_cost\"])\n",
    "    df = df.filter((col(\"sale_price\") >= 0) & (col(\"product_cost\") >= 0))\n",
    "    df = df.dropDuplicates()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_s3(spark: SparkSession, bucket_name: str, years: List[int]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from S3 bucket and filter by year and month.\n",
    "    - Must be Q1 data\n",
    "    - Years is a list of years inputted by user\n",
    "    \"\"\"\n",
    "    base_path = f\"s3a://{bucket_name}/\"\n",
    "    df = spark.read.parquet(base_path)\n",
    "    df = df.filter(\n",
    "        (col(\"created_at_year\").isin(years)) & (col(\"created_at_month\").between(1, 3))\n",
    "    )\n",
    "    \n",
    "    # Print the number of rows grouped by created_at_year\n",
    "    row_counts = df.groupBy(\"created_at_year\").count()\n",
    "    row_counts.show()\n",
    "\n",
    "    df = clean_data(df)\n",
    "\n",
    "    invalid_data = df.filter(\n",
    "        (~col(\"created_at_year\").isin(years)) | (~col(\"created_at_month\").between(1, 3))\n",
    "    )\n",
    "    if invalid_data.count() > 0:\n",
    "        raise ValueError(\"Process contains data outside Q1 or years not included in the years list.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We group by each trafic source\n",
    "2. We aggregate a sum of all items that we wanna calculate (total spent and profits)\n",
    "3. We aggregate a count of all the items \n",
    "4. Calculate the percentage change in the number of purchases between the years period and the last\n",
    "5. Calculate the percentage change in the profit between the years period and the last\n",
    "We could calculate over time, month by month. Since problem asks for Quarter comparison, we leave it at that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms the input data to generate metrics per quarter and traffic source.\n",
    "    Number of purchases and total spent are calculated for each traffic source.\n",
    "    The function returns a DataFrame with the following columns:\n",
    "    - quarter: The quarter (e.g., Q1, Q2, etc.)\n",
    "    - traffic_source: The source of the traffic (e.g., Organic, YouTube, Email, etc.)\n",
    "    - number of purchases: The total amount of purchases from that traffic source\n",
    "    - profit: The profit made from that traffic source (total_spent - total_cost)\n",
    "    - percentage_change_purchases: Percentage change in the number of purchases from the previous quarter\n",
    "    - percentage_change_profit: Percentage change in the profit from the previous quarter\n",
    "    \n",
    "    Input dataset: Q1 of selected years\n",
    "    \"\"\"\n",
    "    # Group by year and traffic source to calculate total metrics\n",
    "    df = df.groupBy(\"created_at_year\", \"traffic_source\").agg(\n",
    "        round(_sum(col(\"sale_price\") - col(\"product_cost\")), 2).alias(\"profit\"),  # Profit is sale_price - product_cost\n",
    "        count(\"product_id\").alias(\"number_of_purchases\")\n",
    "    )\n",
    "\n",
    "    # Define a window partitioned by traffic_source and ordered by year\n",
    "    window_spec = Window.partitionBy(\"traffic_source\").orderBy(\"created_at_year\")\n",
    "\n",
    "    # Calculate percentage change in purchases and profit\n",
    "    df = df.withColumn(\n",
    "        \"percentage_change_purchases\",\n",
    "        (col(\"number_of_purchases\") - lag(\"number_of_purchases\").over(window_spec)) /\n",
    "        lag(\"number_of_purchases\").over(window_spec) * 100\n",
    "    ).withColumn(\n",
    "        \"percentage_change_profit\",\n",
    "        (col(\"profit\") - lag(\"profit\").over(window_spec)) /\n",
    "        lag(\"profit\").over(window_spec) * 100\n",
    "    )\n",
    "    # Round percentage change columns to 2 decimals\n",
    "    df = df.withColumn(\"percentage_change_purchases\", round(col(\"percentage_change_purchases\"), 2)) \\\n",
    "           .withColumn(\"percentage_change_profit\", round(col(\"percentage_change_profit\"), 2))\n",
    "   \n",
    "    #  Lag() function accesses the previous row in the window, so we need to handle null values\n",
    "    df = df.fillna(0, subset=[\"percentage_change_purchases\", \"percentage_change_profit\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2019, 2020, 2021, 2022, 2023, 2024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/hadoop-aws-3.3.4.jar,/content/aws-java-sdk-bundle-1.12.710.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/03/30 23:54:38 WARN Utils: Your hostname, ibum resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/30 23:54:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ibum/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ibum/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ibum/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-10be102f-cc9d-4cf7-ad73-9361f46aee5e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 389ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-10be102f-cc9d-4cf7-ad73-9361f46aee5e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "25/03/30 23:54:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/30 23:54:40 WARN DependencyUtils: Local jar /content/hadoop-aws-3.3.4.jar does not exist, skipping.\n",
      "25/03/30 23:54:40 WARN DependencyUtils: Local jar /content/aws-java-sdk-bundle-1.12.710.jar does not exist, skipping.\n",
      "25/03/30 23:54:40 INFO SparkContext: Running Spark version 3.5.0\n",
      "25/03/30 23:54:40 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 23:54:40 INFO SparkContext: Java version 17.0.6\n",
      "25/03/30 23:54:40 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 23:54:40 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/03/30 23:54:40 INFO ResourceUtils: ==============================================================\n",
      "25/03/30 23:54:40 INFO SparkContext: Submitted application: Read Public S3 Parquet\n",
      "25/03/30 23:54:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/03/30 23:54:40 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/03/30 23:54:40 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/03/30 23:54:40 INFO SecurityManager: Changing view acls to: ibum\n",
      "25/03/30 23:54:40 INFO SecurityManager: Changing modify acls to: ibum\n",
      "25/03/30 23:54:40 INFO SecurityManager: Changing view acls groups to: \n",
      "25/03/30 23:54:40 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/03/30 23:54:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ibum; groups with view permissions: EMPTY; users with modify permissions: ibum; groups with modify permissions: EMPTY\n",
      "25/03/30 23:54:41 INFO Utils: Successfully started service 'sparkDriver' on port 41009.\n",
      "25/03/30 23:54:41 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/30 23:54:41 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/30 23:54:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/03/30 23:54:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/03/30 23:54:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/30 23:54:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-edc3e634-bcdc-4217-b148-0a55d0c9b711\n",
      "25/03/30 23:54:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/03/30 23:54:41 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/03/30 23:54:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/03/30 23:54:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/03/30 23:54:42 ERROR SparkContext: Failed to add file:/content/hadoop-aws-3.3.4.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /content/hadoop-aws-3.3.4.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2100)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2156)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:526)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:526)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:526)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "25/03/30 23:54:42 ERROR SparkContext: Failed to add file:/content/aws-java-sdk-bundle-1.12.710.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /content/aws-java-sdk-bundle-1.12.710.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2100)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2156)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:526)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:526)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:526)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "25/03/30 23:54:42 INFO SparkContext: Added JAR file:///home/ibum/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://10.255.255.254:41009/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:42 INFO SparkContext: Added JAR file:///home/ibum/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://10.255.255.254:41009/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:42 INFO SparkContext: Added JAR file:///home/ibum/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://10.255.255.254:41009/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:42 INFO SparkContext: Added file file:///home/ibum/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at file:///home/ibum/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:42 INFO Utils: Copying /home/ibum/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/org.apache.hadoop_hadoop-aws-3.3.4.jar\n",
      "25/03/30 23:54:42 INFO SparkContext: Added file file:///home/ibum/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at file:///home/ibum/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:42 INFO Utils: Copying /home/ibum/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar\n",
      "25/03/30 23:54:46 INFO SparkContext: Added file file:///home/ibum/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at file:///home/ibum/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:46 INFO Utils: Copying /home/ibum/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar\n",
      "25/03/30 23:54:46 INFO Executor: Starting executor ID driver on host 10.255.255.254\n",
      "25/03/30 23:54:46 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/03/30 23:54:46 INFO Executor: Java version 17.0.6\n",
      "25/03/30 23:54:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/03/30 23:54:46 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@149f2485 for default.\n",
      "25/03/30 23:54:46 INFO Executor: Fetching file:///home/ibum/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:46 INFO Utils: /home/ibum/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar has been previously copied to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/org.apache.hadoop_hadoop-aws-3.3.4.jar\n",
      "25/03/30 23:54:46 INFO Executor: Fetching file:///home/ibum/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:46 INFO Utils: /home/ibum/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar has been previously copied to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar\n",
      "25/03/30 23:54:46 INFO Executor: Fetching file:///home/ibum/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:46 INFO Utils: /home/ibum/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar has been previously copied to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar\n",
      "25/03/30 23:54:46 INFO Executor: Fetching spark://10.255.255.254:41009/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:46 INFO TransportClientFactory: Successfully created connection to /10.255.255.254:41009 after 39 ms (0 ms spent in bootstraps)\n",
      "25/03/30 23:54:46 INFO Utils: Fetching spark://10.255.255.254:41009/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/fetchFileTemp2020587677669738371.tmp\n",
      "25/03/30 23:54:46 INFO Utils: /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/fetchFileTemp2020587677669738371.tmp has been previously copied to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/org.apache.hadoop_hadoop-aws-3.3.4.jar\n",
      "25/03/30 23:54:46 INFO Executor: Adding file:/tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/org.apache.hadoop_hadoop-aws-3.3.4.jar to class loader default\n",
      "25/03/30 23:54:46 INFO Executor: Fetching spark://10.255.255.254:41009/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:46 INFO Utils: Fetching spark://10.255.255.254:41009/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/fetchFileTemp5222896018445586464.tmp\n",
      "25/03/30 23:54:49 INFO Utils: /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/fetchFileTemp5222896018445586464.tmp has been previously copied to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar\n",
      "25/03/30 23:54:54 INFO Executor: Adding file:/tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to class loader default\n",
      "25/03/30 23:54:54 INFO Executor: Fetching spark://10.255.255.254:41009/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1743389680623\n",
      "25/03/30 23:54:54 INFO Utils: Fetching spark://10.255.255.254:41009/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/fetchFileTemp4401943952080140652.tmp\n",
      "25/03/30 23:54:54 INFO Utils: /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/fetchFileTemp4401943952080140652.tmp has been previously copied to /tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar\n",
      "25/03/30 23:54:57 INFO Executor: Adding file:/tmp/spark-9ac6a7c5-1ad3-4162-8e20-f7b2fec68077/userFiles-15dcdf50-2796-4be6-bb9e-e1ceb661a439/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to class loader default\n",
      "25/03/30 23:54:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36733.\n",
      "25/03/30 23:54:57 INFO NettyBlockTransferService: Server created on 10.255.255.254:36733\n",
      "25/03/30 23:54:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/03/30 23:54:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.255.255.254, 36733, None)\n",
      "25/03/30 23:54:57 INFO BlockManagerMasterEndpoint: Registering block manager 10.255.255.254:36733 with 434.4 MiB RAM, BlockManagerId(driver, 10.255.255.254, 36733, None)\n",
      "25/03/30 23:54:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.255.255.254, 36733, None)\n",
      "25/03/30 23:54:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.255.255.254, 36733, None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Public S3 Parquet\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "# Set the logging level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # If not, pyspark commonly prints any log that it finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"da-exercise/thelook_ecommerce/purchases/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 data goes from January 1st to March 31 (included)\n",
    "We see that we indeed have data outside our scope (April's data). We filter for all rows from January (1) to March (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------------------+---+------+-----------+--------+-------------+--------+----------+------------+----------------+------------------+--------------------+------------+----------+-------------------+----------------+---------------+--------------+--------------------+\n",
      "|user_id|first_name|last_name|               email|age|gender|postal_code|    city|      country|order_id|product_id|product_cost|product_category|product_department|        product_name|order_status|sale_price|         created_at|created_at_month|created_at_year|traffic_source|          session_id|\n",
      "+-------+----------+---------+--------------------+---+------+-----------+--------+-------------+--------+----------+------------+----------------+------------------+--------------------+------------+----------+-------------------+----------------+---------------+--------------+--------------------+\n",
      "|  28950|     Scott|  Sanders|scottsanders@exam...| 26|     M|   359-0025|      所|        Japan|   36057|     26166|       14.01|       Underwear|               Men|Craft Men's Cool ...|    Returned|      30.0|2024-03-24 07:06:37|               3|           2024|       YouTube|604b4396-ed75-4a4...|\n",
      "|  45142| Nathaniel|      Lin|nathaniellin@exam...| 19|     M|  85555-000|  Palmas|       Brasil|   56354|     25056|       17.19|           Socks|               Men|Polo Ralph Lauren...|    Returned|      30.0|2024-03-21 22:00:51|               3|           2024|       Adwords|2a4929cd-8fdd-410...|\n",
      "|  57939|   Michael|   Malone|michaelmalone@exa...| 50|     M|      94801|Richmond|United States|   72413|     26758|       11.85|  Sleep & Lounge|               Men|Croft & BarrowÂ® ...|    Returned|      30.0|2024-03-23 08:37:49|               3|           2024|         Email|762203fa-a401-41b...|\n",
      "|  94948|    Marvin|   Walker|marvinwalker@exam...| 15|     M|     215124|  Ningbo|        China|  119145|     20836|       15.75|           Jeans|               Men|Lee Men's Premium...|    Returned|      30.0|2024-03-02 00:06:40|               3|           2024|       Adwords|f9844c93-5b1d-439...|\n",
      "|  95439|   Jeffery|Rodriguez|jefferyrodriguez@...| 32|     M|       1790|Affligem|      Belgium|  119793|     26565|       11.01|  Sleep & Lounge|               Men|State O Maine - M...|    Returned|      30.0|2024-03-30 01:22:02|               3|           2024|         Email|a10a45e1-bb29-43e...|\n",
      "+-------+----------+---------+--------------------+---+------+-----------+--------+-------------+--------+----------+------------+----------------+------------------+--------------------+------------+----------+-------------------+----------------+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_raw = read_data_from_s3(spark, bucket_name, years)\n",
    "data_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------+-------------------+---------------------------+------------------------+\n",
      "|created_at_year|traffic_source|   profit|number_of_purchases|percentage_change_purchases|percentage_change_profit|\n",
      "+---------------+--------------+---------+-------------------+---------------------------+------------------------+\n",
      "|           2019|       Adwords|  1486.38|                 49|                        0.0|                     0.0|\n",
      "|           2020|       Adwords| 18650.94|                615|                     1155.1|                 1154.79|\n",
      "|           2021|       Adwords| 38513.75|               1217|                      97.89|                   106.5|\n",
      "|           2022|       Adwords| 67450.07|               2170|                      78.31|                   75.13|\n",
      "|           2023|       Adwords|115143.54|               3692|                      70.14|                   70.71|\n",
      "|           2024|       Adwords|265011.25|               8332|                     125.68|                  130.16|\n",
      "|           2019|         Email|  2647.25|                 83|                        0.0|                     0.0|\n",
      "|           2020|         Email| 27535.73|                885|                     966.27|                  940.16|\n",
      "|           2021|         Email| 55891.49|               1865|                     110.73|                  102.98|\n",
      "|           2022|         Email| 99546.48|               3336|                      78.87|                   78.11|\n",
      "|           2023|         Email|173443.74|               5474|                      64.09|                   74.23|\n",
      "|           2024|         Email|400814.02|              12834|                     134.45|                  131.09|\n",
      "|           2019|      Facebook|   955.45|                 20|                        0.0|                     0.0|\n",
      "|           2020|      Facebook|  5869.62|                202|                      910.0|                  514.33|\n",
      "|           2021|      Facebook| 13567.78|                395|                      95.54|                  131.15|\n",
      "|           2022|      Facebook| 23029.59|                747|                      89.11|                   69.74|\n",
      "|           2023|      Facebook| 36063.37|               1223|                      63.72|                    56.6|\n",
      "|           2024|      Facebook| 89209.02|               2907|                     137.69|                  147.37|\n",
      "|           2019|       Organic|   218.48|                  7|                        0.0|                     0.0|\n",
      "|           2020|       Organic|  3239.09|                101|                    1342.86|                 1382.56|\n",
      "+---------------+--------------+---------+-------------------+---------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = transform_data(data_raw)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No other data cleaning processes are needed\n",
    "data_cleaned = data.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to report_2019_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "output_path = \"report_2019_2024.csv\"\n",
    "data_cleaned.write.csv(output_path, header=True)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
